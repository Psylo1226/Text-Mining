{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Wykorzystać model BERT do klasyfikacji tekstu, aby rozpoznać, z której powieści (Anna Karenina lub Jane Eyre) pochodzi dany fragment tekstu.\n",
    "\n",
    "\n",
    "1. Przygotuj dane wejściowe:\n",
    "   - Podziel teksty obu powieści na fragmenty o stałej długości (np. 100 słów lub 5 zdań).\n",
    "   - Przypisz etykiety: `0` dla *Anna Karenina*, `1` dla *Jane Eyre*.\n",
    "2. Skorzystaj z modelu `BertForSequenceClassification` do klasyfikacji tekstu.\n",
    "3. Przeprowadź fine-tuning modelu na przygotowanym zbiorze danych.\n",
    "4. Oceń skuteczność modelu na zbiorze testowym."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Wykorzystać model BERT do analizy toksyczności komentarzy.\n",
    "\n",
    "\n",
    "1. Załaduj zbiór danych o toksycznych komentarzach(dostępny na platformie).\n",
    "2. Skorzystaj z modelu `BertForSequenceClassification` i przeprowadź fine-tuning na tym zbiorze danych.\n",
    "3. Oceń model na zbiorze testowym i zinterpretuj wyniki.\n",
    "4. Przeprowadź analizę – znajdź komentarze, które model zaklasyfikował jako toksyczne, a które jako neutralne.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ZAD1\n",
    "\n",
    "with open('anna_karenina.txt', 'r', encoding='utf-8') as f:\n",
    "    anna_karenina_text = f.read()\n",
    "\n",
    "with open('jane_eyre.txt', 'r', encoding='utf-8') as f:\n",
    "    jane_eyre_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\KamilSarzyniak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "anna_sentences = nltk.sent_tokenize(anna_karenina_text)\n",
    "jane_sentences = nltk.sent_tokenize(jane_eyre_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\ufeffThe Project Gutenberg eBook of Anna Karenina\\n    \\nThis ebook is for the use of anyone anywhere in the United States and\\nmost other parts of the world at no cost and with almost no restrictions\\nwhatsoever.', 'You may copy it, give it away or re-use it under the terms\\nof the Project Gutenberg License included with this ebook or online\\nat www.gutenberg.org.', 'If you are not located in the United States,\\nyou will have to check the laws of the country where you are located\\nbefore using this eBook.']\n",
      "['\\ufeffThe Project Gutenberg eBook of Jane Eyre: An Autobiography\\n    \\nThis ebook is for the use of anyone anywhere in the United States and\\nmost other parts of the world at no cost and with almost no restrictions\\nwhatsoever.', 'You may copy it, give it away or re-use it under the terms\\nof the Project Gutenberg License included with this ebook or online\\nat www.gutenberg.org.', 'If you are not located in the United States,\\nyou will have to check the laws of the country where you are located\\nbefore using this eBook.']\n"
     ]
    }
   ],
   "source": [
    "print(anna_sentences[:3])\n",
    "print(jane_sentences[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_chunks(text, chunk_size=100):\n",
    "    words = nltk.word_tokenize(text)\n",
    "\n",
    "    chunks = [words[i:i + chunk_size] for i in range(0, len(words), chunk_size)]\n",
    "    \n",
    "    return [\" \".join(chunk) for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "anna_chunks = split_into_chunks(anna_karenina_text)\n",
    "jane_chunks = split_into_chunks(jane_eyre_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\ufeffThe Project Gutenberg eBook of Anna Karenina This ebook is for the use of anyone anywhere in the United States and most other parts of the world at no cost and with almost no restrictions whatsoever . You may copy it , give it away or re-use it under the terms of the Project Gutenberg License included with this ebook or online at www.gutenberg.org . If you are not located in the United States , you will have to check the laws of the country where you are located before using this eBook . Title : Anna Karenina Author :', 'graf Leo Tolstoy Translator : Constance Garnett Release date : July 1 , 1998 [ eBook # 1399 ] Most recently updated : April 9 , 2023 Language : English Credits : David Brannan , Andrew Sly and David Widger * * * START OF THE PROJECT GUTENBERG EBOOK ANNA KARENINA * * * [ Illustration ] ANNA KARENINA by Leo Tolstoy Translated by Constance Garnett Contents PART ONE PART TWO PART THREE PART FOUR PART FIVE PART SIX PART SEVEN PART EIGHT PART ONE Chapter 1 Happy families are all alike ; every unhappy family is unhappy in its']\n",
      "['\\ufeffThe Project Gutenberg eBook of Jane Eyre : An Autobiography This ebook is for the use of anyone anywhere in the United States and most other parts of the world at no cost and with almost no restrictions whatsoever . You may copy it , give it away or re-use it under the terms of the Project Gutenberg License included with this ebook or online at www.gutenberg.org . If you are not located in the United States , you will have to check the laws of the country where you are located before using this eBook . Title : Jane', 'Eyre : An Autobiography Author : Charlotte Brontë Illustrator : F. H. Townsend Release date : March 1 , 1998 [ eBook # 1260 ] Most recently updated : May 2 , 2023 Language : English Credits : David Price * * * START OF THE PROJECT GUTENBERG EBOOK JANE EYRE : AN AUTOBIOGRAPHY * * * JANE EYRE AN AUTOBIOGRAPHY by Charlotte Brontë _ILLUSTRATED BY F. H. TOWNSEND_ London SERVICE & PATON 5 HENRIETTA STREET 1897 _The Illustrations_ _in this Volume are the copyright of_ SERVICE & PATON , _London_ TO W. M. THACKERAY , ESQ. , This Work']\n"
     ]
    }
   ],
   "source": [
    "print(anna_chunks[:2])\n",
    "print(jane_chunks[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "anna_labels = [0] * len(anna_chunks)\n",
    "jane_labels = [1] * len(jane_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = anna_chunks + jane_chunks\n",
    "labels = anna_labels + jane_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\ufeffThe Project Gutenberg eBook of Anna Karenina This ebook is for the use of anyone anywhere in the United States and most other parts of the world at no cost and with almost no restrictions whatsoever . You may copy it , give it away or re-use it under the terms of the Project Gutenberg License included with this ebook or online at www.gutenberg.org . If you are not located in the United States , you will have to check the laws of the country where you are located before using this eBook . Title : Anna Karenina Author :', 'graf Leo Tolstoy Translator : Constance Garnett Release date : July 1 , 1998 [ eBook # 1399 ] Most recently updated : April 9 , 2023 Language : English Credits : David Brannan , Andrew Sly and David Widger * * * START OF THE PROJECT GUTENBERG EBOOK ANNA KARENINA * * * [ Illustration ] ANNA KARENINA by Leo Tolstoy Translated by Constance Garnett Contents PART ONE PART TWO PART THREE PART FOUR PART FIVE PART SIX PART SEVEN PART EIGHT PART ONE Chapter 1 Happy families are all alike ; every unhappy family is unhappy in its']\n",
      "[0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(text_data[:2])\n",
    "print(labels[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples, padding=\"max_length\", truncation=True, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = tokenize_function(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[101, 1996, 2622, 9535, 11029, 26885, 1997, 4698, 8129, 3981, 2023, 26885, 2003, 2005, 1996, 2224, 1997, 3087, 5973, 1999, 1996, 2142, 2163, 1998, 2087, 2060, 3033, 1997, 1996, 2088, 2012, 2053, 3465, 1998, 2007, 2471, 2053, 9259, 18971, 1012, 2017, 2089, 6100, 2009, 1010, 2507, 2009, 2185, 2030, 2128, 1011, 2224, 2009, 2104, 1996, 3408, 1997, 1996, 2622, 9535, 11029, 6105, 2443, 2007, 2023, 26885, 2030, 3784, 2012, 7479, 1012, 9535, 11029, 1012, 8917, 1012, 2065, 2017, 2024, 2025, 2284, 1999, 1996, 2142, 2163, 1010, 2017, 2097, 2031, 2000, 4638, 1996, 4277, 1997, 1996, 2406, 2073, 2017, 2024, 2284, 2077, 2478, 2023, 26885, 1012, 2516, 1024, 4698, 8129, 3981, 3166, 1024, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 22160, 6688, 2000, 4877, 29578, 11403, 1024, 15713, 11721, 26573, 2102, 2713, 3058, 1024, 2251, 1015, 1010, 2687, 1031, 26885, 1001, 16621, 2683, 1033, 2087, 3728, 7172, 1024, 2258, 1023, 1010, 16798, 2509, 2653, 1024, 2394, 6495, 1024, 2585, 24905, 7229, 1010, 4080, 18230, 1998, 2585, 15536, 11818, 2099, 1008, 1008, 1008, 2707, 1997, 1996, 2622, 9535, 11029, 26885, 4698, 8129, 3981, 1008, 1008, 1008, 1031, 14614, 1033, 4698, 8129, 3981, 2011, 6688, 2000, 4877, 29578, 5421, 2011, 15713, 11721, 26573, 2102, 8417, 2112, 2028, 2112, 2048, 2112, 2093, 2112, 2176, 2112, 2274, 2112, 2416, 2112, 2698, 2112, 2809, 2112, 2028, 3127, 1015, 3407, 2945, 2024, 2035, 11455, 1025, 2296, 12511, 2155, 2003, 12511, 1999, 2049, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_data['input_ids'][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    tokenized_data['input_ids'], labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5296 1325\n"
     ]
    }
   ],
   "source": [
    "print(len(train_texts), len(test_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_dict({\n",
    "    'input_ids': train_texts,\n",
    "    'labels': train_labels\n",
    "})\n",
    "\n",
    "test_dataset = Dataset.from_dict({\n",
    "    'input_ids': test_texts,\n",
    "    'labels': test_labels\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1986' max='1986' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1986/1986 2:17:38, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.106200</td>\n",
       "      <td>0.036517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.023700</td>\n",
       "      <td>0.023809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.014300</td>\n",
       "      <td>0.024506</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1986, training_loss=0.03800494781313586, metrics={'train_runtime': 8263.3194, 'train_samples_per_second': 1.923, 'train_steps_per_second': 0.24, 'total_flos': 1045077111889920.0, 'train_loss': 0.03800494781313586, 'epoch': 3.0})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.02450556308031082, 'eval_runtime': 175.0457, 'eval_samples_per_second': 7.569, 'eval_steps_per_second': 0.948, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ZAD2\n",
    "\n",
    "df = pd.read_csv(\"sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"labels\"] = df[\"severe_toxicity\"].apply(lambda x: 1 if x > 0.5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"comment_text\", \"labels\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df[\"comment_text\"].tolist(), df[\"labels\"].tolist(), test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_dict({\"text\": train_texts, \"labels\": train_labels})\n",
    "test_dataset = Dataset.from_dict({\"text\": test_texts, \"labels\": test_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 8000/8000 [00:13<00:00, 572.94 examples/s]\n",
      "Map: 100%|██████████| 2000/2000 [00:03<00:00, 586.22 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KamilSarzyniak\\anaconda3\\envs\\AI_DL_ML\\lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3000/3000 2:36:37, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3000, training_loss=0.0024265777938999237, metrics={'train_runtime': 9403.5344, 'train_samples_per_second': 2.552, 'train_steps_per_second': 0.319, 'total_flos': 1578666332160000.0, 'train_loss': 0.0024265777938999237, 'epoch': 3.0})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 9.175793820759282e-06, 'eval_runtime': 193.9151, 'eval_samples_per_second': 10.314, 'eval_steps_per_second': 1.289, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trainer.predict(test_dataset).predictions\n",
    "pred_labels = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_comments = [text for text, label in zip(test_texts, pred_labels) if label == 1]\n",
    "neutral_comments = [text for text, label in zip(test_texts, pred_labels) if label == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liczba komentarzy toksycznych: 0\n",
      "Liczba komentarzy neutralnych: 2000\n",
      "\n",
      "Przykłady toksycznych komentarzy:\n",
      "[]\n",
      "\n",
      "Przykłady neutralnych komentarzy:\n",
      "[\"Ya, its almost like we need to do something besides lay off all the state workers. Entitlements cost almost all of that $3.7 billion. So we have 2 choices. reduce entitlements and spend my money on my family, or increase taxes and spend my money on someone else's.\", \"Trump is under investigation for his Russian ties, and he just proved that he's a White Supremacist sympathizer, if he isn't one himself.\", \"That argument makes no sense, WM. Society moves forward, those that choose not to shouldn't think that those that did have to pay for their defunct lifestyle.\", 'Well then I certainly hope you are going to go to your local university the next time a men\\'s rights group or conservative speaker is coming and the SJW\\'s (or \"peacocks\" as Scott Adams calls them) are screaming and shouting and making threats to try and shut the event down. If you are at Dalhousie the young woman you mention is likely to be there with her pals trying to prevent the invited speaker(s) from being heard so maybe you\\'ll have a chance to explain the hypocrisy to her.', 'Key words: \"mythical and mystical\" and opening shipping routes. China recognizes climate change and acting on it. Think importing Chinese tourists (not those seeking wildlife trophy), exporting Alaskan Native made cultural items that benefit bush artisans, and cooperative renewable clean energy solutions.....Don\\'t focus on old energy economies alone. Think bigger, Alaska. Glad you had this friendly visitor. Mandarin and high tech energy system university investments anyone?']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Liczba komentarzy toksycznych: {len(toxic_comments)}\")\n",
    "print(f\"Liczba komentarzy neutralnych: {len(neutral_comments)}\")\n",
    "\n",
    "# Przykłady toksycznych komentarzy\n",
    "print(\"\\nPrzykłady toksycznych komentarzy:\")\n",
    "print(toxic_comments[:5])\n",
    "\n",
    "# Przykłady neutralnych komentarzy\n",
    "print(\"\\nPrzykłady neutralnych komentarzy:\")\n",
    "print(neutral_comments[:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_DL_ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
